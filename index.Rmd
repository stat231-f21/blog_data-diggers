---
title: | 
  [![kickstarter](kickstarter.jpg){width=2in}](https://www.kickstarter.com/?ref=nav)
subtitle: "Analyzing and Modeling Kickstarter Data: What are the drivers of success for Kickstarter campaigns?"
author: "Dan Dachille and Karla Munoz"
date: "November 30, 2021"
output:
  rmdformats::robobook:
    thumbnails: false
    highlight: "kate"
---

```{r setup, include = FALSE}
library(tidyverse)
library(kableExtra) 
library(tidytext)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(plotly)
library(gganimate)
library(gifski)
library(cowplot)
library(Stat2Data) # for emplogitplot1()
library(ggformula) # for gf_percents()
library(lmtest) # lrt test
library(gt) # to create a pretty conf matrix
library(MLmetrics) # for precision/recall

# Set code chunk defaults 
knitr::opts_chunk$set(echo = FALSE, 
                      mesage = FALSE,
                      warning = FALSE,
                      fig.align = "center")
# set reproducible randomness
set.seed(123)
# Set R environment options
options(knitr.kable.NA = '')
```

# Introduction

## What is Kickstarter?

Kickstarter is a rewards-based online crowdfunding platform founded in 2009. Traditionally, entrepreneurs would pitch their ideas to venture capital firms, banks, or angel investors in an effort to raise capital. Crowdfunding a way for start-ups to circumvent the traditional path and raise capital from a large number of individual investors. Crowdfunding typically leverages the power of online social networks to get the word out and attract investors. Kickstarter is “rewards-based” because investors&mdash;also known as “backers”&mdash;receive nonmonetary rewards rather than equity in exchange for supporting a company. Project creators can set reward thresholds so that backers who pledge more money get larger rewards. For most projects, backers receive the product that the company plans to make in exchange for a large reward. 

When starting their project, creators choose one of the fifteen categories, set a campaign length, establish a funding goal, among other options. If the project does not reach its goal by the end of the campaign, backers keep their money. If a project is successful, Kickstarter releases the funds to creators so that they can bring their idea to fruition.  Some of the most successful companies that were funded on the platform include [Peloton](https://www.kickstarter.com/projects/568069889/the-peloton-bike-bring-home-the-studio-cycling-exp), [Oculus VR](https://www.kickstarter.com/projects/1523379957/oculus-rift-step-into-the-game), and [Allbirds](https://www.kickstarter.com/projects/3over7/the-wool-runners-no-socks-no-smell).

## What was our motivation?

While Kickstarter makes it easy to explore projects on their website, they do not have many tools that let you explore the underlying trends in Kickstarter campaigns. We were interested in digging deeper to understand what attributes the most successful campaigns have. If we could uncover the features of Kickstarter campaigns that drive success, then we could help creators optimize their projects. 

From writing a strong project description to determining the best launch day, we want to inspire creators as they embark on their creative endeavors. As crowdfunding continues to grow in popularity, more and people will benefit from analyses like these. 

## How did we get the data?

A company called [Web Robots](https://webrobots.io/kickstarter-datasets/) has a web crawler that scrapes data from [Kickstarter's website](https://www.kickstarter.com/) every month. We aggregated 58 csv files that were scraped by their web crawler in December of 2020. Since the columns are the same across csv files, we simply read in all of the csv files and bound the rows together.

```{r printed read files, eval = FALSE, echo = TRUE}
# get the name of the folder containing the Kickstarter csv files
root <- "kickstarter_raw_data"
# get the names of all of the Kickstarter csv files
files <- dir(root, pattern='csv$', recursive = TRUE, full.names = TRUE) 
# read in all of the csv files and bind them to a data frame
all_data <- files %>% 
  map_dfr(read_csv)
```

Once we had all the data in a single data frame, we could start making visualizations and models, right? Well... not so fast. There was a substantial amount of data wrangling and cleaning that had to be done before we could get to the fun stuff. Our initial csv file had 213,564 Kickstarter campaigns with 24,135 duplicates. 

After removing the duplicate campaigns, we filtered for only the campaigns that succeeded or failed, removing those that were ongoing, suspended, or canceled. We also fixed various issues in the columns of the data set, such as inconsistencies in the conversion of the goal and pledged amounts to USD, an inaccurate country column, and a JSON formatted category column, just to name a few. After cleaning the data, we were left with 170,536 Kickstarter campaigns in our data frame. If you like digging into data and are eager to learn more about our data wrangling process, please check out the [code](https://github.com/stat231-f21/blog_data-diggers/blob/079c15b0d7ad6721d54d129b0aa8c50ac8eb18f6/data-wrangling.Rmd) on github. 


# Success by State and Category

<!--- iframe allows for more customization than include_app(), doesn't cut off edges -->
<iframe src="https://ddachille23.shinyapps.io/MapGadgetApp/" height="600" width="720" style="border: 1px solid #464646;" data-external="1"></iframe>

The interactive table makes it easy to compare the success rates of the 15 Kickstarter categories in all states or a specific US state. It ranks the Kickstarter categories based on their success rates in descending order. For all US states, comic book projects have the highest success rate with over 87% of comic book campaigns reaching their funding goal. On the other hand, journalism projects have the lowest success rate in all US states, with a measly 24% of projects reaching their funding goal. 

To compare success rates for a specific category across states, we can use the map and select a category. Darker states have higher success rates in the selected category. For all categories, campaigns in Vermont, Massachusetts, and New York have the highest success rates while campaigns in South Dakota, Florida, and Mississippi have the lowest success rates. If we select Photography as a category for example, we can see that Nevada has the highest success rate by far, since it is a much darker shade of green than the other states. This may be explained by Nevada being home to the [Burning Man Festival](https://en.wikipedia.org/wiki/Burning_Man), for which successful Photography campaigns like [this](https://www.kickstarter.com/projects/zippylomax/dusty-playground?ref=discovery_category_most_funded&term=burning%20man) are centered around. 

We used the [`ggplot2`](https://ggplot2.tidyverse.org/index.html) and [`ggiraph`](https://davidgohel.github.io/ggiraph/index.html) packages to make the map, and the [`miniUI`](https://github.com/rstudio/miniUI) package to add the table and map symbols in the app.  

# Text Analysis of Kickstarter Names and Blurbs

## Text Length and Outcome {.tabset .tabset-fade .tabset-pills}

```{r echo = FALSE}
# read in data
kickstarter <- read.csv("kickstarter_2020.csv")
# wrangle
text <- kickstarter %>%
  select(blurb, name, outcome, main_category)

small_data <- kickstarter %>%
  select(blurb, name)

# remove stop words 
data(stop_words)
wordcloud_data <- text %>%
   unnest_tokens(output = word, input = blurb) %>%
   anti_join(stop_words, by = "word")

# join text
new_text <- inner_join(wordcloud_data, small_data, by = "name")
```

### Name Length

```{r echo = FALSE}
# blurb length by outcome
length_data <- text %>% 
  mutate (blurb_length = nchar(blurb),
          name_length = nchar(name))
length_summary <- length_data %>% 
  group_by(outcome) %>% 
  summarise(mean_blurb_length = mean(blurb_length), n = n(),
            mean_name_length = round(mean(name_length), 1)) %>% 
  mutate(mean_blurb_length = round(mean_blurb_length, 1),
         outcome = as.factor(outcome))
# reorder levels of outcome factor
length_summary$outcome <- factor(length_summary$outcome, levels=rev(levels(length_summary$outcome)))

# name length plot
ggplot(length_summary, aes(x = mean_name_length, y = outcome)) + 
  geom_bar(stat = "identity", aes(fill = outcome)) +
  theme_minimal() + labs(x = "Mean Name Length (characters)", 
                         y = "Outcome", title = "Name Length by Outcome") + 
  geom_label(aes(label = mean_name_length)) +
  scale_fill_manual(values=c("#05ce78", "#ed4752")) +
  theme(legend.position = "none")
```


### Blurb Length

```{r echo = FALSE}
# blurb length plot
ggplot(length_summary, aes(x = mean_blurb_length, y = outcome)) + 
  geom_bar(stat = "identity", aes(fill = outcome)) +
  theme_minimal() + labs(x = "Mean Blurb Length (characters)", 
                         y = "Outcome", title = "Blurb Length by Outcome") + 
  geom_label(aes(label = mean_blurb_length)) +
  scale_fill_manual(values=c("#05ce78", "#ed4752")) +
  theme(legend.position = "none")

```

## {-}

The name of a Kickstarter campaign refers to its title and the blurb refers to the short description below the title. The bar graph of name length shows that successful campaigns have a mean name length that is 3.7 characters longer than unsuccessful campaigns. There does not appear to be much of a difference in blurb length between successful and failed campaigns. 

## Sentiment Analysis of Kickstarter Blurbs {.tabset .tabset-fade .tabset-pills}

```{r non-eval printed sentiment chunk, eval = FALSE, echo = TRUE}
# get Bing sentiment scores
bing_scores <- blurb_sentiment_bing %>% 
  group_by(name, outcome) %>% 
  summarize(
    sentiment_score = sum(sentiment == "positive") - 
      sum(sentiment == "negative")
    )
# get Afinn sentiment scores
afinn_scores <- blurb_sentiment_fin %>% 
  group_by(name, outcome) %>% 
  summarize(sentiment_score = sum(value))
```

We used two different lexicons to perform our text analysis. First, we used the Bing lexicon, which categorizes words as either positive or negative. In order to calculate a sentiment score for each campaign, we added up the number of positive and negative words in their blurbs and then found the difference. Next, we used the Afinn lexicon, which categorizes words on a scale from -5 to 5, -5 being the most negative and 5 the most positive. To get a sentiment score for each campaign using the Afinn lexicon, we summed the score for each word the campaign's blurb. 


### Bing Lexicon

```{r echo = FALSE, message = FALSE}
# using bing lexicon (pos or neg)
bing <- get_sentiments("bing")

# join in bing and get blurb length
blurb_sentiment <- new_text %>% 
  inner_join(bing) 

# creating bing sentiment score
tt_bing <- blurb_sentiment %>% 
  group_by(name, outcome) %>% 
  summarize(
    sentiment_score = sum(sentiment == "positive") - 
      sum(sentiment == "negative"),
    tPos = sum(sentiment == "positive"), 
    tNeg = sum(sentiment == "negative")
    )
# make table
bing_tbl <- tt_bing %>% 
  group_by(outcome) %>% 
  summarise(mean_sent_score = mean(sentiment_score),
            sd = sd(sentiment_score),
            median = median(sentiment_score))
bing_tbl_pretty <- bing_tbl %>% 
  mutate(mean_sent_score = round(mean_sent_score, 3)) %>% 
  select(outcome, mean_sent_score) %>% 
  rename(Outcome = outcome, 
         "Mean Sentiment Score (Bing)" = mean_sent_score) %>% 
  kbl() %>%
  kable_material("hover")

# sentiment score faceted plot
ggplot(data = tt_bing, aes(x = sentiment_score, fill = outcome)) + 
  geom_histogram(bins = 15) +
  facet_grid(~outcome) + 
  labs(title = "Blurb Sentiment Score by Outcome", 
       x = "Sentiment Score (Bing lexicon)") + 
  scale_fill_manual(values = c("#ed4752", "#05ce78")) +
  theme_bw() +
  theme(legend.position = "none") 
bing_tbl_pretty
```

### Afinn Lexicon

```{r echo = FALSE, message = FALSE}
# using afinn lexicon
afinn <- get_sentiments("afinn")

blurb_sentiment_fin <- new_text %>% 
  inner_join(afinn)

# get afinn sentiment score
tt_afinn <- blurb_sentiment_fin %>% 
  group_by(name, outcome) %>% 
  summarize(sentiment_score = sum(value))

# make table
afinn_tbl <- tt_afinn %>% 
  group_by(outcome) %>% 
  summarise(mean_sent_score = mean(sentiment_score),
            sd = sd(sentiment_score),
            median = median(sentiment_score)) 
afinn_tbl_pretty <- afinn_tbl %>% 
  mutate(mean_sent_score = round(mean_sent_score, 3)) %>% 
  select(outcome, mean_sent_score) %>% 
  rename(Outcome = outcome, 
         "Mean Sentiment Score (Afinn)" = mean_sent_score) %>% 
  kbl() %>%
  kable_material("hover")


# sentiment score faceted plot
ggplot(data = tt_afinn, aes(x = sentiment_score, fill = outcome)) + 
  geom_histogram(bins = 15) +
  facet_grid(~outcome) + 
  labs(title = "Blurb Sentiment Score by Outcome",
       x = "Sentiment Score (Afinn lexicon)") + 
  scale_fill_manual(values = c("#ed4752", "#05ce78")) +
  theme_bw() +
  theme(legend.position = "none") 
afinn_tbl_pretty
```

## {-}

The blurb sentiment graphs show that regardless of lexicon, the sentiment of blurbs is very similar between successful and failed campaigns. We can see that the distribution of sentiment scores is almost identical across outcomes for both lexicons. It appears that failed campaigns had a slightly higher mean sentiment score than successful campaigns for both lexicons, meaning that on average, the words in their blurbs are marginally more positive.   

## Comparison Word Cloud of Kickstarter Blurbs

<iframe src="https://ddachille23.shinyapps.io/WordCloudApp/" height="780" width="720" style="border: 1px solid #464646;" data-external="1"></iframe>

The comparison word cloud compares the relative frequency of words across failed and successful campaigns in the chosen category and sizes the words based on that relative frequency. For example, the words "enamel" and "pins" show up in both failed and successful art campaigns but show up many more times in successful campaigns, so they appear as large green words on the successful side. This would suggest that enamel pin projects tend to do well on Kickstarter. A comparison word cloud provides much more utility than a normal word cloud in this context because it allows you to see which words show up the most in successful and failed campaigns for each category. This word cloud provides granular information as to which campaigns tend to succeed in a way that other visualizations cannot. 

We created this visualization using the `comparison.cloud()` function from the [`wordcloud`](https://github.com/cran/wordcloud) package. 


# Univariate Analysis of Potential Predictors

This section examines both the categorical and quantitative variables that describe the Kickstarter campaigns in our data set. We analyze variables that are known when the campaign is first launched and could potentially be used to predict a campaign's chance of reaching its goal. We don't include amount funded as a potential predictor since it is not known until after the campaign ends. Selected categorical variables include staff pick status, launch day, launch month, launch year, and category. Quantitative variables are campaign length and goal size.

## Categorical Variables 

### Bar Graphs {.tabset .tabset-fade .tabset-pills}

#### Staff Pick
```{r Staff Pick}
# make table
staff_pick_success <- kickstarter %>% 
  select(staff_pick,
         outcome) %>% 
  group_by(staff_pick) %>% 
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n)
# make graph
ggplot(data = staff_pick_success, aes(x = staff_pick, y = success_rate)) + 
  geom_bar(stat = "identity", fill = "#05ce78") + 
  labs(title = "Success Rate by Staff Pick",
       x = "Staff Pick",
       y = "Success Rate") +
  theme_minimal() +
  geom_label(aes(label = round(success_rate, 3))) 
```

#### Launch Day
```{r Date Wrangling}
# wrangling dates
date_data <- kickstarter %>%
  select(c("launched_at", "outcome", "proportion_funded",
           "pct_funded", "campaign_length")) %>%
  separate(launched_at, c("date", "time"), sep = " ") %>%
  mutate(month = months(as.Date(date)),
         day = weekdays(as.Date(date)),
         year = year(as.Date(date))) 
```

```{r Launch Day}
# dataset for day
day_data <- date_data %>%
  group_by(day) %>%
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) 
# day barplot
ggplot(data = day_data, aes(x = day, y = success_rate)) +
  geom_bar(stat = "identity", fill = "#05ce78") +
  labs(title = "Success Rate by Launch Day",
       x = "Day",
       y = "Success Rate") +
  scale_x_discrete(limits = c("Sunday", "Monday", "Tuesday", "Wednesday",
                              "Thursday", "Friday", "Saturday")) +
  geom_label(aes(label = round(success_rate, 3))) +
  theme_minimal()
```

#### Launch Month
```{r Launch Month}
# dataset for month
month_data <- date_data %>%
  group_by(month) %>%
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n)
# creating bar plot
p2 <- month_data %>%
  ggplot(aes(x = month, y = success_rate)) +
  geom_bar(stat = "identity", fill = "#05ce78") + coord_flip() +
  labs(title = "Success Rate by Launch Month", 
       x = "Month",
       y = "Success Rate") +
  scale_x_discrete(limits = rev(c("January", "February", "March", 
                                  "April", "May", "June", 
                                  "July", "August", "September",
                                  "October", "November", "December"))) +
  geom_label(aes(label = round(success_rate, 3))) +
  theme_minimal()
p2
```

<!-- #### Launch Year (shown in time series graph, not needed here)--> 
```{r include = FALSE}
# dataset for year
year_data_bar <- date_data %>%
  group_by(year) %>%
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) %>% 
  mutate(year = as.factor(year))
# reverse order of the years
year_data_bar$year <- factor(year_data_bar$year, 
                         levels=rev(levels(year_data_bar$year)))
# creating bar plot
year_data_bar %>%
  ggplot(aes(x = year, y = success_rate)) +
  geom_bar(stat = "identity", fill = "#05ce78") + coord_flip() +
  labs(title = "Success Rate by Launch Year", 
       x = "Year",
       y = "Success Rate") +
  geom_label(aes(label = round(success_rate, 3))) +
  theme_minimal()
```


#### Category
```{r Category}
# make table
cat_success <- kickstarter %>% 
  select(main_category,
         outcome) %>% 
  group_by(main_category) %>% 
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) %>% 
  mutate(main_category = as.factor(main_category))
# reverse order of the categories
cat_success$main_category <- factor(cat_success$main_category, 
                         levels=rev(levels(cat_success$main_category)))
# make bar graph
ggplot(data = cat_success, aes(x = main_category, y = success_rate)) + 
  geom_bar(stat = "identity", fill = "#05ce78") + 
  labs(title = "Success Rate by Category",
       x = "Category",
       y = "Success Rate") +
  theme_minimal() +
  coord_flip() +
  geom_label(aes(label = round(success_rate, 3))) 
```


### {-}

- Staff Pick: 51.8% of campaigns that were not staff picks were successful, while 90.2% of campaigns that were staff picks were successful.
- Launch Day: Tuesday is the best day to launch a campaign, as 60.7% of campaigns launched on Tuesday were successful. Saturday is the worst, since only 53.9% of campaigns launched on that day were successful.
- Launch Month: October is the best month to launch a campaign, as 59.7% of campaigns launched in October were successful. December is the worst, since only 50.7% of campaigns launched in December were successful.
- Category: The top 3 categories were comics, publishing, and games: 87.4% of comic campaigns were successful, 70.2% of publishing campaigns were successful, and 69.4% of game campaigns were successful. The bottom 3 categories were Journalism, Crafts, and Food: 24.1% of journalism campaigns were successful, 28.1% of craft campaigns were successful, and 29.3% of food campaigns were successful.



### Year
```{r}
# dataset for year
year_data <- date_data %>%
  group_by(year) %>%
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) 

# creating line plot for year 
gif_plot_year <- ggplot(year_data, aes(x = year, y = success_rate)) +
  geom_line(color = "#05ce78") + geom_point() +
  labs(title = "Trend in Outcome of Kickstarter Projects by Year",
       x = "Year",
       y = "Success Rate") +
  transition_reveal(year) +
  theme_minimal() + 
  geom_text(aes(label = scales::percent(success_rate, accuracy = 1),
                vjust = -2), show.legend = FALSE) +
  scale_x_discrete(limits = 2009:2020)

# saving line plot into a gif
animate(gif_plot_year, duration = 6, fps = 40, 
        width = 450, height = 400, renderer = gifski_renderer())
#anim_save("line-plot.gif")

```

Animated with the [`gganimate`](https://gganimate.com/) package, the above time series plot shows that the success rates of Kickstarter campaigns in its early years remained constant at above a 75% success rate. However, after 2013, there was a significant dip from 80% to a whopping 44% success rate by 2015. So how do we explain this drop in the trend of successful campaigns from 2013-2015? 

Laura Lewis, an Amazon data scientist who examined Kickstarter success trends throughout the years, created a time series plot with the total number of launched projects each year from 2009-2019. She found that by 2014, Kickstarter saw a significant rise in campaigns with a peak of over 4000 projects launched in 2014 (Lewis, 2019). Thus, the proportion of successful projects decreased as the number of launched projects increased. Nevertheless, the site has seen considerable improvement in success rates in recent years while the number of launched projects have remained fairly constant since 2016. 



## Quantitative Variables  

### Campaign Length {.tabset .tabset-fade .tabset-pills}

From 2009-2011, Kickstarter creators were able to set campaign lengths of up to 90 days. However, in 2011, Kickstarter claimed that through their own research, they found that projects lasting longer than 60 days were less successful than projects with shorter campaign lengths, Thus, after 2011, campaign lengths were recommended to be at most 30-60 days (Strickler, 2011). 

In this section, we analyze the validity of Kickstarter's findings: Do shorter campaign lengths correlate with higher success rates on Kickstarter projects? 

#### 2011

```{r, message = FALSE}
# dataset for campaign length (2011)
camplength1 <- date_data %>%
  group_by(campaign_length, year) %>%
  filter(year == 2011) %>%
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) 

# plot for campaign length 2011
ggplot(camplength1, aes(x = campaign_length, y = success_rate, size = n)) +
  geom_smooth(method = "glm", color = "#05ce78") + geom_point() +
  labs(title = "Success of Kickstarter Projects by Campaign Length (2011)",
       x = "Campaign Length (in days)",
       y = "Success Rate",
       size = "Number of Campaigns") +
  theme_minimal()
```

#### 2012-2020

```{r, message = FALSE}
# for 2012-2020
camplength2 <- date_data %>%
  group_by(campaign_length, year) %>%
  filter(year > 2011, campaign_length <= 60) %>%
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) 

# plot for campaign length 2012-2020
ggplot(camplength2, aes(x = campaign_length, y = success_rate, size = n)) +
  geom_smooth(method = "glm", color = "#05ce78") + geom_point() +
  labs(title = "Success of Kickstarter Projects by Campaign Length (2011-2020)",
       x = "Campaign Length (in days)",
       y = "Success Rate",
       size = "Number of Campaigns") +
  facet_wrap(~ year) +   
  scale_size_continuous(limits = c(1, 10000)) +
  theme_minimal()
```

### {-}

In 2011, a year prior to the change in campign length, there is a moderately strong, negative, and roughly linear association between campaign length and success rate. So as the campaign length increases, the success rate decreases. 

In the multi-panel plot, created with the `facet_wrap()` function, we see this trend from 2011 continue as the number of campaigns begin to increase significantly. Most of the campaigns in 2012-2020 set a campaign length of approximately 30 days and are more successful than campaigns with much longer campaign lengths. We predict this is because 30 days suggests a greater level of urgency for a project to be backed, especially if it is something that prompts a lot of interest. 

### Goal Size
```{r}
# dataset for goal size 
goal_size <- kickstarter %>% 
  select(goal_usd, outcome) %>% 
  # cut up funding goals
  group_by(cuts=cut(goal_usd, c(-1, 100, 1000, 10000, 
                                100000, 1000000, Inf))) %>% 
  summarise(n = n(),
            success_rate = sum(outcome=="successful")/n) %>% 
  mutate(cuts = as.factor(cuts),
         cuts_clean = case_when(cuts == "(-1,100]" ~ "0-100",
                                cuts == "(100,1e+03]" ~ "100-1000",
                                cuts == "(100,1e+03]" ~ "100-1000",
                                cuts == "(1e+03,1e+04]" ~ "1000-10000",
                                cuts == "(1e+04,1e+05]" ~ "10000-100000",
                                cuts == "(1e+05,1e+06]" ~ "100000-1000000",
                                cuts == "(1e+06,Inf]" ~ ">1000000"))
# make graph
ggplot(goal_size, aes(x = reorder(cuts_clean, -success_rate), 
                      y = success_rate)) +
 geom_col(fill = "#05ce78") +
  labs(title = "Success of Kickstarter Projects by Goal Size",
       x = "Goal Size ($)",
       y = "Success Rate") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  geom_label(aes(label = round(success_rate, 3))) 

```

This graph shows that as goal size increases, success rate decreases. This makes sense since it is easier for a campaign to reach their goal if they don't ask for a lot of money. Campaigns with a goal from \$0-\$100 USD had a 73.7% success rate while campaigns with a goal over \$1,000,000 had a meager 1.5% success rate. 


# Multivariable Thinking With a Logistic Regression Model
```{r prepare data}
# cast categorical predictors into factors, mutate outcome into 0 = failed, 1 = successful
kickstarter_init <- kickstarter %>% 
  mutate(outcome = ifelse(outcome == "successful", 1, 0),
         main_category = as.factor(main_category),
         sub_category = as.factor(sub_category),
         city = as.factor(city),
         state = as.factor(state),
         country_corrected = as.factor(country_corrected),
         launched_at = date(launched_at))

# subset data to relevant variables
kickstarter_subset <- kickstarter_init %>% 
  select(backers_count, country_display_name_corrected, 
         launched_at, spotlight, staff_pick, 
         outcome, goal_usd, pledged_usd, 
         campaign_length, main_category, sub_category, 
         city, state, proportion_funded, name, blurb) %>% 
  mutate(launch_month = month(launched_at),
         launch_day = wday(launched_at),
         name_length = str_length(name),
         blurb_length = str_length(blurb))
```

## Train-Test Split
```{r 70-30 data split, echo = TRUE}
sample <- sample(c(TRUE, FALSE), nrow(kickstarter_subset), 
                 replace = TRUE, prob = c(0.7,0.3))
train <- kickstarter_subset[sample, ]
test <- kickstarter_subset[!sample, ]
```

We begin by splitting our data set into training and testing sets. 70% of the Kickstarter campaigns are put into the training set while the other 30% are put into the testing set. Splitting the data into training and testing set allows us to see how well our model preforms on an out of sample data set. In other words, it allows us to test our model's predictive ability on data that it has never seen before. 

## Predictors
1) `goal_usd` (Quantitative, $): The chosen amount of money in USD a company needs to raise to have a successful campaign.
2) `campaign_length` (Quantitative, Days): The chosen length of the campaign, ranging from 0-90 in 2009-2011, and 0-60 in 2012-2020.
3) `name_length` (Quantitative, Characters): The length of the startup's name.
4) `staff_pick` (Categorical, TRUE/FALSE): Whether or not the project was selected by the staff at Kickstarter to be included in the "Projects we Love" section.
5) `main_category` (Categorical): The main category of the campaign. There are 14 main categories (Art, Comics, Crafts, ... etc.).
6) `launch_day` (Categorical): The day of the week in which the campaign was launched.
7) `launch_month` (Categorical): The month in which the campaign was launched.

## Conditions for Logistic Regression 
- Is outcome binary? Yes outcome is binary since a Kickstarter campaign can either have a failed or successful outcome.
- Linearity: Checked for quantitative predictors below, automatic for categorical predictors.
- Independence of observations:  It is reasonable to assume that Kickstarter campaigns independent. 
- Randomness: We know that we didn't capture all Kickstarter campaigns from 2009-2020, but we will assume that the 170,000 scraped campaigns are a random subset of the population.

## EDA

### Response Variable - Kickstarter Campaign Outcome
```{r}
outcome_summary_tbl <- train %>% group_by(outcome) %>% summarise(n = n()) %>% 
  mutate(freq = round(n/sum(n), 4),
         outcome = ifelse(outcome == 1, "Successful", "Failed")) %>% 
  rename("Outcome" = outcome, "Number of Campaigns" = n, 
         "Frequency of Campaigns" = freq)
outcome_summary_tbl%>%  
  kbl() %>%
  kable_material("hover")
```

56.9% of all Kickstarter campaigns in the training set were successful.

### Quantitative Predictors - Check Linearity of Predictors and log-odds {.tabset .tabset-fade .tabset-pills}

#### Goal
```{r}
# QUANT PREDICTOR 1 - GOAL
emplogitplot1(outcome ~ goal_usd, data = train, ngroups = 15, xlab = "goal ($)", main = "Goal Empirical Logit Plot")
```

#### Sqrt(Goal)
```{r}
# SQRT GOAL
# transform variable - for BOTH training and testing set
train <- train %>% mutate(sqrt_goal_usd = sqrt(goal_usd))
test <- test %>% mutate(sqrt_goal_usd = sqrt(goal_usd))

emplogitplot1(outcome ~ sqrt_goal_usd, data = train, 
              ngroups = 15, xlab = "sqrt(goal)", 
              main = "Square Root Transformed Goal Empirical Logit Plot")
```

#### Name Length
```{r}
# QUANT PREDICTOR 2 - NAME LENGTH
emplogitplot1(outcome ~ name_length, data = train, ngroups = 15, 
              xlab = "name length (characters)", 
              main = "Name Length Empirical Logit Plot")
```

#### Campaign Length
```{r}
# QUANT PREDICTOR 3 - CAMPAIGN LENGTH
emplogitplot1(outcome ~ campaign_length, data = train, 
              xlab = "campaign length (days)", 
              ngroups = "all", 
              main = "Campaign Length Emprical Logit Plot")
```

### {-}

Funding goal does not initially have a linear relationship with the log odds of the outcome, so we applied a square root transformation. A square root transformation will inflate smaller goals and stabilize larger ones, which is helpful here because of the right skewed distribution of Kickstarter goals. The name length has a linear relationship with the log odds of the outcome. The empirical logit plot for the campaign length looks concerning in terms of both linearity and strength of the relationship.


## Model Fitting

```{r model fit, echo = TRUE}
mod.full <- glm(outcome ~ sqrt_goal_usd + name_length + campaign_length + 
                  staff_pick + launch_day + launch_month + 
                  main_category, data = train, family = "binomial")
mod.final <- glm(outcome ~ sqrt_goal_usd + name_length + 
                   campaign_length + staff_pick + launch_day + 
                   main_category, data = train, family = "binomial")
```

```{r model summary, eval = FALSE}
summary(mod.full)
summary(mod.final)
```

Using a Wald test for each predictor, all predictors are significant at an alpha level of 0.05, with the exception of launch_month and a single level of the main category predictor. We will remove launch month since it is not significant. There does not appear to be a significant difference between the dance category and our baseline category, art. In order to see if we should keep the main category predictor in our model, we will perform a chi-squared test.

```{r echo = TRUE}
car::Anova(mod.final, type = "III")
```

The chi-squared test returns a significant p-value an alpha level of 0.05 for the main category predictor, meaning that it significantly helps in the prediction of the log(odds) of the outcome. Therefore, we will keep main category in the final model. The chi-squared test also shows that all other predictors in our final model are significant, so we will proceed with this model.

## Final Model Interpretation

The predictor with the largest effect size is staff pick. The indicator variable that corresponds to staff pick being true has a coefficient of 2.62 with an extremely small p-value. An intercept of 2.62 means that the estimated odds of success for a campaign that is a staff pick is 13.7 times higher than a campaign that is not a staff pick, after accounting for the effect of the other predictors (since $e^\hat\beta = odds\space ratio$). The predictor with the next largest effect size is the indicator variable that corresponds to the comic category, with a coefficient of 1.48 and an extremely small p-value. An intercept of 1.48 means that the estimated odds of success for a comic book campaign is 4.40 times higher than a campaign with the baseline category, art, after accounting for the effect of the other predictors.  

## Model Evaluation

```{r}
# get predicted values
test.predicted.mod.final <-  predict(mod.final, 
                                     newdata = test,
                                     type = "response")
conf.matrix <- table(test$outcome, test.predicted.mod.final > 0.5)

# make pretty confusion matrix
pretty.conf.matrix <- as.data.frame(conf.matrix) %>% 
  pivot_wider(names_from = Var2, values_from = Freq) %>% 
  mutate(Var1 = as.character(Var1),
         Var1 = ifelse(Var1 == "0", "True Failure", "True Success")) %>% 
  gt() %>% 
  cols_label(Var1 = " ", `FALSE` = "Predicted Failure", 
             `TRUE` =  "Predicted Success") %>% 
  tab_header(title = md("Confusion Matrix")) %>% 
  tab_options(heading.title.font.size = 18) %>% 
  tab_style(
    style = list(
      cell_fill(color = "#05ce78"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `FALSE`,
      rows = 1
    )
  ) %>% 
  tab_style(
    style = list(
      cell_fill(color = "#05ce78"),
      cell_text(style = "italic")
      ),
    locations = cells_body(
      columns = `TRUE`,
      rows = 2
    )
  ) %>% 
  tab_style(
    style = list(
      cell_fill(color = "#ed4752",
                 alpha = 0.9)
      ),
    locations = cells_body(
      columns = `FALSE`,
      rows = 2
    )
  ) %>% 
  tab_style(
    style = list(
      cell_fill(color = "#ed4752",
                alpha = 0.9)
      ),
    locations = cells_body(
      columns = `TRUE`,
      rows = 1
    )
  )
pretty.conf.matrix
```

```{r eval = FALSE}
Accuracy(ifelse(test.predicted.mod.final >= .5, 1, 0), test$outcome)
# true pos rate
Precision(test$outcome, 
          ifelse(test.predicted.mod.final >= .5, 1, 0), 
          positive = 1) 
# sensitivity
Recall(test$outcome, 
       ifelse(test.predicted.mod.final >= .5, 1, 0),
       positive = 1) 
```

$$Accuracy =  \frac{(TP + TN)}{(TP + TN + FP + FN)} = \frac{(24055 + 12808)}{(24055 + 12808 + 9477 + 4958)} = 0.7186$$

The model is 71.86% accurate, meaning that it predicts the correct outcome 71.86% of the time.

$$Precision =  \frac{(TP)}{(TP + FP)} = \frac{(24055)}{(24055 + 9477)} = 0.7174$$


The model's precision is 71.74%, meaning that 71.74% of the campaigns predicted to be successful were truly successful. In other words, when it predicts a campaign will be successful, it is correct 71.74% of the time.

$$Recall =  \frac{(TP)}{(TP + FN)} = \frac{(24055)}{(24055 + 4958)} = 0.8291$$


The model's recall is 82.91%, meaning that of all the successful campaigns, the model correctly predicted them to be successful 82.91% of the time (true positive rate). In other words, it correctly identifies 82.91% of all successful campaigns. 

$$Specificity = \frac{(TN)}{(TN + FP)} = \frac{(12808)}{(12808 + 9477)} = 0.5747$$
The model's specificity is 57.47% meaning that of all the failed campaigns, the model correctly predicted them to fail 57.47% of the time (true negative rate). In other words, it correctly identifies 57.47% of all failed campaigns.  

## Model Takeaways

Given the features in our data set, we are satisfied with the accuracy of our final model. Our model's classification abilities were similar to Amazon data scientist Laura Lewis' final model, as her model had a 71% accuracy score (less than 1% different). Like Laura's model, our model is better at predicting successful campaigns than failed ones, since the model's true positive rate is much larger than its true negative rate. Our model could be used by creators to help them gauge how likely they are to reach their goal. Our model is also useful for backers, because it will help them get an idea as to how likely they are to receive their rewards.

The model's accuracy could potentially be improved in the future by adding other information not captured in our data set such as campaign [reward](https://www.kickstarter.com/help/handbook/rewards) pricing structure. 

# Prediction App

<iframe src="https://ddachille23.shinyapps.io/LogitModelApp/" height="650" width="720" style="border: 1px solid #464646;" data-external="1"></iframe>

The prediction app makes it easy for the user to interact with our model in real time. The user simply enters the information about the Kickstarter campaign they are interested in starting or analyzing and the app tells them how likely that campaign is to succeed. It also helps the user see how certain attributes have a large impact on a campaigns chance of succeeding. When you change the staff pick option from true to false, the odds of succeeding drops by a factor of 13.7. For example, our model predicts that a 30 day photography campaign called "Amy D Photography" with a $5,000 goal launched on a Monday that is a staff pick has a 82.93% chance of succeeding while the same campaign has a 26.14% chance if it is not a staff pick. Converting from probability to odds, we see that the odds of the staff pick campaign succeeding is 4.8582 while the odds of the non-staff pick campaign succeeding is 0.3539. The ratio of those two odds gives us 13.7, and taking the natural log of 13.7 gives us the coefficient of the staff pick predictor in our model, 2.62.

As far as we know, no other application exists that gives insights into a Kickstarter campaign's likelihood of succeeding based on its unique parameters. We could see Kickstarter implementing an application like this to help creators maximize their chances of succeeding.

# Conclusion

Whether you're a prospective Kickstarter creator, data scientist, or curious onlooker, we hope this blog fostered a greater understanding of the factors that go into creating a successful Kickstarter campaign.

So after reading this blog, you've decided on what your Kickstarter campaign will look like: Being from Vermont, you will create a campaign under the Publishing category and set a campaign goal of $2000 to be reached within 30 days. You will launch it on a Tuesday, maybe give it a fairly long title with 37 characters and ensure that your blurb somehow fits the words _book_, _children's_, and _illustrated_. And you really, really hope that your project gets chosen for staff pick. You've got the perfect plan! Your campaign is bound to succeed...or is it?

It is important to acknowledge the limitations of our analysis. For example, we did not have access to data on the types of rewards creators set in their campaigns. We also did not account for the demographics of backers, fads that made some projects more popular than others over a short interval of time, or the probability of a company failing after a successful Kickstarter campaign.

While some of the results we conveyed can guide your campaign, do not let these findings control every aspect of your Kickstarter process. Ultimately, the probability of success might just depend on launching the right project at the right time, or who knows, maybe Tuesday really is the only good day to start a campaign. 


# References

Lewis, Laura. (2019, April 9). _Using machine learning to predict Kickstarter success_. Towards Data Science. https://towardsdatascience.com/using-machine-learning-to-predict-kickstarter-success-e371ab56a743

Kickstarter. (2009). _About_. https://www.kickstarter.com/about

Kickstarter (2009). _Building Rewards_. https://www.kickstarter.com/help/handbook/rewards

Strickler, Yancey. (2011, June 17). _Shortening the Maximum Project Length_. The Kickstarter Blog. https://www.kickstarter.com/blog/shortening-the-maximum-project-length 

James, Gareth (2021, August 4) _Logistic Regression_. UC Business Analytics R Programming Guide.
https://uc-r.github.io/logistic_regression
